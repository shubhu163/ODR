{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b16447fee9974036833a6e70f0471196":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_398465100a0c4f2aa9a6b501f85597ee","IPY_MODEL_de2aea87e5ff41a78c7960a834c6af8e","IPY_MODEL_f6d972cfedba4c14be65f4d82523386e"],"layout":"IPY_MODEL_06aefbb282e741e695d901cfcad742e1"}},"398465100a0c4f2aa9a6b501f85597ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92a689b5fce845e692f27454cd9c5cf3","placeholder":"​","style":"IPY_MODEL_d12d62ce57dc483989ae1543f8a0420c","value":"100%"}},"de2aea87e5ff41a78c7960a834c6af8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c1bea4573d94cbfb65840aa6a290033","max":47,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82e4000b37ee4fc9a03b87e39b37aecd","value":47}},"f6d972cfedba4c14be65f4d82523386e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91eb13912d1c46568cf2b44fdde9836d","placeholder":"​","style":"IPY_MODEL_b100fc8ded324fb78cfc810545f0f344","value":" 47/47 [1:19:02&lt;00:00, 100.99s/it]"}},"06aefbb282e741e695d901cfcad742e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92a689b5fce845e692f27454cd9c5cf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d12d62ce57dc483989ae1543f8a0420c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c1bea4573d94cbfb65840aa6a290033":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e4000b37ee4fc9a03b87e39b37aecd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91eb13912d1c46568cf2b44fdde9836d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b100fc8ded324fb78cfc810545f0f344":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torch import nn\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, datasets\nfrom pathlib import Path\nfrom timeit import default_timer as timer\nfrom torch.utils.data import Subset\n","metadata":{"id":"mMTv72MSsxkm","execution":{"iopub.status.busy":"2023-04-29T07:33:17.734119Z","iopub.execute_input":"2023-04-29T07:33:17.734989Z","iopub.status.idle":"2023-04-29T07:33:21.052722Z","shell.execute_reply.started":"2023-04-29T07:33:17.734940Z","shell.execute_reply":"2023-04-29T07:33:21.051094Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RcClqujrsxkn","outputId":"88fcc4d2-22bb-47b2-d0d5-4f18d66619e2","execution":{"iopub.status.busy":"2023-04-29T07:33:21.055040Z","iopub.execute_input":"2023-04-29T07:33:21.055624Z","iopub.status.idle":"2023-04-29T07:33:21.071568Z","shell.execute_reply.started":"2023-04-29T07:33:21.055583Z","shell.execute_reply":"2023-04-29T07:33:21.070186Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"torch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"yr5jEVOksxko","outputId":"74c7f9fc-026a-460c-c8b5-39f618ed26c8","execution":{"iopub.status.busy":"2023-04-29T07:33:21.073683Z","iopub.execute_input":"2023-04-29T07:33:21.074172Z","iopub.status.idle":"2023-04-29T07:33:21.191047Z","shell.execute_reply.started":"2023-04-29T07:33:21.074133Z","shell.execute_reply":"2023-04-29T07:33:21.189816Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"train_dir = Path( '/kaggle/input/super-data/Super_Dataset/train')\ntest_dir = Path('/kaggle/input/super-data/Super_Dataset/val')\n","metadata":{"id":"jdBX-Sdssxko","execution":{"iopub.status.busy":"2023-04-29T07:33:21.193923Z","iopub.execute_input":"2023-04-29T07:33:21.194679Z","iopub.status.idle":"2023-04-29T07:33:21.199856Z","shell.execute_reply.started":"2023-04-29T07:33:21.194641Z","shell.execute_reply":"2023-04-29T07:33:21.198542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_transform = transforms.Compose([\n    # Resize our images to 64x64\n    transforms.Resize(size=(320, 320)),\n    # Flip the images randomly on the horizontal \n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(20),\n    #transforms.RandomInvert(0.4),\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor()\n])\n","metadata":{"id":"iQqEZdQ-sxko","execution":{"iopub.status.busy":"2023-04-29T07:33:21.201573Z","iopub.execute_input":"2023-04-29T07:33:21.202325Z","iopub.status.idle":"2023-04-29T07:33:21.210773Z","shell.execute_reply.started":"2023-04-29T07:33:21.202290Z","shell.execute_reply":"2023-04-29T07:33:21.209200Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\ntrain_dataset = datasets.ImageFolder(train_dir, transform=data_transform, target_transform=None)\ntest_dataset = datasets.ImageFolder(test_dir, transform=data_transform)\n","metadata":{"id":"0Uxt_s6osxkp","execution":{"iopub.status.busy":"2023-04-29T07:33:21.215015Z","iopub.execute_input":"2023-04-29T07:33:21.215377Z","iopub.status.idle":"2023-04-29T07:33:23.628268Z","shell.execute_reply.started":"2023-04-29T07:33:21.215342Z","shell.execute_reply":"2023-04-29T07:33:23.626664Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())\n\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count())\n\n","metadata":{"id":"8osC20Oysxkp","execution":{"iopub.status.busy":"2023-04-29T07:33:23.630085Z","iopub.execute_input":"2023-04-29T07:33:23.630477Z","iopub.status.idle":"2023-04-29T07:33:23.639734Z","shell.execute_reply.started":"2023-04-29T07:33:23.630438Z","shell.execute_reply":"2023-04-29T07:33:23.637689Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class_names= train_dataset.classes\nclass_names","metadata":{"id":"CJV_yz2IQWMT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5faa862c-0ba6-4ec8-f07f-543c546ca090","execution":{"iopub.status.busy":"2023-04-29T07:33:23.642086Z","iopub.execute_input":"2023-04-29T07:33:23.642478Z","iopub.status.idle":"2023-04-29T07:33:23.654691Z","shell.execute_reply.started":"2023-04-29T07:33:23.642416Z","shell.execute_reply":"2023-04-29T07:33:23.653651Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['Age_Macular_Degeneration',\n 'Cataract',\n 'Diabetic Retinopathy',\n 'Glaucoma',\n 'Myopia',\n 'Normal']"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n          \n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results","metadata":{"id":"4YkEkdJmsxkp","execution":{"iopub.status.busy":"2023-04-29T07:33:23.656731Z","iopub.execute_input":"2023-04-29T07:33:23.658099Z","iopub.status.idle":"2023-04-29T07:33:23.682701Z","shell.execute_reply.started":"2023-04-29T07:33:23.658062Z","shell.execute_reply":"2023-04-29T07:33:23.681497Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from typing import List, Tuple\n\nfrom PIL import Image\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (512, 512),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = data_transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -> prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    \n    print(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n","metadata":{"id":"6hctCoHYJAqM","execution":{"iopub.status.busy":"2023-04-29T07:33:23.686520Z","iopub.execute_input":"2023-04-29T07:33:23.686914Z","iopub.status.idle":"2023-04-29T07:33:23.701574Z","shell.execute_reply.started":"2023-04-29T07:33:23.686852Z","shell.execute_reply":"2023-04-29T07:33:23.700412Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"weights = torchvision.models.EfficientNet_B6_Weights.IMAGENET1K_V1\n\nmodel = torchvision.models.efficientnet_b6(weights=weights).to(device)\n\nfor params in model.parameters():\n    params.requires_grad = True\n\n# n_inputs = model.fc.in_features\nmodel.classifier = nn.Sequential(nn.Dropout(p=0.4),\n                                nn.Linear(2304, 2304),\n                                nn.ReLU(),\n                                nn.Dropout(p=0.4),\n                                nn.Linear(2304, 6),\n                                nn.LogSigmoid()).to(device)\n\n                               \nfor name, child in model.named_children():\n    for name2, params in child.named_parameters():\n        params.requires_grad = True\n\nweights = [1/186, 1/793, 1/1893, 1/773, 1/162, 1/2095]\nweights = torch.FloatTensor(weights).cuda()\nloss_fn = nn.CrossEntropyLoss(weight=weights)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4, weight_decay=1e-4)\n\nepochs = 16\ntrain_start = timer()\nmodel_results = train(model,train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\ntrain_end = timer()\n\nprint(f\"Total time on {device}: {train_end - train_start}\")\n\n# custom_image_path = Path('/content/drive/MyDrive/NF.jpg')\n\n# pred_and_plot_image(model=model, image_path=custom_image_path, class_names= class_names)\n\nsave_path = Path(\"/kaggle/working/Super_model_29_04_2023.EfficientnetB6_V2_8-Batch.pt\")\n\ntorch.save(model,save_path)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":900,"referenced_widgets":["b16447fee9974036833a6e70f0471196","398465100a0c4f2aa9a6b501f85597ee","de2aea87e5ff41a78c7960a834c6af8e","f6d972cfedba4c14be65f4d82523386e","06aefbb282e741e695d901cfcad742e1","92a689b5fce845e692f27454cd9c5cf3","d12d62ce57dc483989ae1543f8a0420c","9c1bea4573d94cbfb65840aa6a290033","82e4000b37ee4fc9a03b87e39b37aecd","91eb13912d1c46568cf2b44fdde9836d","b100fc8ded324fb78cfc810545f0f344"]},"id":"ZUQi4lXdsxkr","outputId":"66f198f3-4365-4795-a357-bd6592f7cca2","execution":{"iopub.status.busy":"2023-04-29T07:33:52.154831Z","iopub.execute_input":"2023-04-29T07:33:52.155411Z","iopub.status.idle":"2023-04-29T09:16:46.040929Z","shell.execute_reply.started":"2023-04-29T07:33:52.155371Z","shell.execute_reply":"2023-04-29T09:16:46.039639Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b6_lukemelas-c76e70fd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b6_lukemelas-c76e70fd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/165M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f487349d27747bc92dd4322adefe96c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79c44c9b0324313b2d19e159886c353"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 0.9729 | train_acc: 0.6060 | test_loss: 0.8308 | test_acc: 0.6703\nEpoch: 2 | train_loss: 0.6071 | train_acc: 0.7357 | test_loss: 0.7250 | test_acc: 0.7200\nEpoch: 3 | train_loss: 0.4810 | train_acc: 0.7829 | test_loss: 0.6680 | test_acc: 0.7572\nEpoch: 4 | train_loss: 0.4205 | train_acc: 0.7976 | test_loss: 0.6407 | test_acc: 0.7635\nEpoch: 5 | train_loss: 0.3797 | train_acc: 0.8115 | test_loss: 0.7181 | test_acc: 0.7355\nEpoch: 6 | train_loss: 0.3448 | train_acc: 0.8289 | test_loss: 0.5587 | test_acc: 0.7965\nEpoch: 7 | train_loss: 0.2868 | train_acc: 0.8498 | test_loss: 0.7109 | test_acc: 0.7942\nEpoch: 8 | train_loss: 0.2632 | train_acc: 0.8597 | test_loss: 0.6034 | test_acc: 0.7971\nEpoch: 9 | train_loss: 0.2769 | train_acc: 0.8631 | test_loss: 0.6172 | test_acc: 0.8008\nEpoch: 10 | train_loss: 0.2382 | train_acc: 0.8806 | test_loss: 0.7192 | test_acc: 0.7904\nEpoch: 11 | train_loss: 0.2103 | train_acc: 0.8958 | test_loss: 0.7788 | test_acc: 0.7744\nEpoch: 12 | train_loss: 0.2017 | train_acc: 0.8974 | test_loss: 0.6292 | test_acc: 0.8033\nEpoch: 13 | train_loss: 0.1840 | train_acc: 0.9060 | test_loss: 0.6952 | test_acc: 0.8190\nEpoch: 14 | train_loss: 0.1831 | train_acc: 0.9107 | test_loss: 0.7071 | test_acc: 0.7613\nEpoch: 15 | train_loss: 0.1602 | train_acc: 0.9141 | test_loss: 0.8737 | test_acc: 0.7710\nEpoch: 16 | train_loss: 0.1647 | train_acc: 0.9172 | test_loss: 0.8052 | test_acc: 0.7915\nTotal time on cuda: 6155.907377253\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = torch.load(\"/content/drive/MyDrive/Multiple_model_25_04_2023.Resnet_V2_8-Batch.pt\")\n# custom_image_path = Path('/content/drive/MyDrive/MYO.jpg')\n\n# pred_and_plot_image(model=model, image_path=custom_image_path, class_names= class_names )","metadata":{"id":"wqBtk7dqi0Pj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab0be852-e47c-4843-ac03-1ba0f7a127d2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"Pred: Normal | Prob: 1.000\n"}]},{"cell_type":"code","source":"import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet18\nfrom sklearn.metrics import confusion_matrix\n\nval_dir= '/kaggle/input/data-1/Super_Dataset/test'\ntest_dataset = datasets.ImageFolder(test_dir, transform=data_transform)\n\n# Load the pretrained ResNet18 model\n# model = torch.load(\"/content/drive/MyDrive/Multiple_model_25_04_2023.Resnet_V2_8-Batch.pt\")\n\n# Set device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the device\nmodel.to(device)\n\n# Evaluate the model on the test dataset\nmodel.eval()\npredictions = []\ntrue_labels = []\nwith torch.no_grad():\n    for inputs, labels in test_dataset:\n        inputs = inputs.to(device)\n        labels = torch.tensor(labels).to(device)\n        outputs = model(inputs.unsqueeze(0))\n        _, predicted = torch.max(outputs.data, 1)\n        predictions.extend(predicted.cpu().numpy())\n        true_labels.extend((labels.item(),))\n\n# Compute the confusion matrix\ncm = confusion_matrix(true_labels, predictions)\n\n# Compute the accuracy\naccuracy = cm.diagonal().sum() / cm.sum()\n\nprint(\"Confusion matrix:\")\nprint(cm)\nprint(\"Accuracy:\", accuracy)","metadata":{"id":"so4OLf9FUPpg","execution":{"iopub.status.busy":"2023-04-29T09:19:11.271546Z","iopub.execute_input":"2023-04-29T09:19:11.272284Z","iopub.status.idle":"2023-04-29T09:20:39.698135Z","shell.execute_reply.started":"2023-04-29T09:19:11.272236Z","shell.execute_reply":"2023-04-29T09:20:39.696903Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Confusion matrix:\n[[ 39   0   2   0   0  12]\n [  0 238   0  11   0   3]\n [ 14   2 406  16   3  99]\n [  2   7   0 231   0   6]\n [  0   0   0   0  42   4]\n [ 27  26  66  98   2 495]]\nAccuracy: 0.7839005942733658\n","output_type":"stream"}]}]}